# Llama Stack Server Dependencies
# This is a minimal pyproject.toml for the Llama Stack server container
# Used by Containerfile.llama-stack

[project]
name = "llama-stack-server"
version = "0.1.0"
description = "Llama Stack Agentic AI Server"
requires-python = ">=3.12"
dependencies = [
    # Core Llama Stack - pinned to 0.3.5 (fixes env var handling bugs)
    "llama-stack==0.3.5",
    "llama-stack-client==0.3.5",
    
    # MCP (Model Context Protocol) for tool runtime
    "mcp>=1.23.1",
    
    # Vector database - pinned to 2.4.x to avoid mmap issues with BM25 sparse fields in OpenShift
    # Version 2.5+ has BM25 sparse field support that doesn't work in restricted containers
    "pymilvus[milvus_lite]>=2.4.0,<2.5.0",
    
    # Database for persistence
    "aiosqlite>=0.20.0",
    
    # HTTP client and OpenAI provider
    "httpx>=0.28.0",
    "openai>=1.82.0",
    
    # Ollama client (required for remote::ollama provider)
    "ollama>=0.6.1",
    
    # YAML config parsing
    "pyyaml>=6.0.3",
    
    # Character encoding detection (required for file processing in vector stores)
    "chardet>=5.2.0",
]
